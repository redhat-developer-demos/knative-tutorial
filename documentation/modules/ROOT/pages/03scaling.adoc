= Scaling
include::_attributes.adoc[]

At the end of this chapter you will be able to:

* Understand what scale-to-zero is and why it's important.
* Configure the scale-to-zero time period.
* Configure the autoscaler.
* Understand types of autoscaling strategies.
* Enable concurrency based autoscaling.
* Configure a minimum number of replicas for a service.

[#scaling-prerequisite]
== Prerequisites
include::partial$prereq-cli.adoc[]

[#scaling-build-containers]
== Build Containers
include::partial$build-containers.adoc[tag=greeter]

[#scaling-deploy-service]
== Deploy Service

The following snippet shows how a Knative service YAML will look like:

[.text-center]
.link:{github-repo}/{basics-repo}/knative/service.yaml[service.yaml]
[source,yaml,linenums]
----
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: greeter
spec:
  runLatest: #<1>
    configuration:
      revisionTemplate:
        spec:
          container:
            image: dev.local/rhdevelopers/greeter:0.0.1 #<2>
----

<1> Configures the Knative service to always run the latest revision of the deployment.
<2> It is very important that the image is a fully qualified docker image name, including a tag. For more details on this <<faq-q2,Question 2 of FAQ>>.

Navigate to the tutorial chapter's `knative` folder:

[source,bash,subs="+macros,+attributes"]
----
cd $TUTORIAL_HOME/01-basics/knative
----

The service can be deployed using the command:

[source,bash,subs="+macros,+attributes"]
----
kubectl apply -n knativetutorial -f link:{github-repo}/{basics-repo}/knative/service.yaml[service.yaml]
----

.(OR)

[source,bash,subs="+macros,+attributes"]
----
oc apply -n knativetutorial -f link:{github-repo}/{basics-repo}/knative/service.yaml[service.yaml]
----

After the deployment was successful of the service we should see a kubernetes deployment called `greeter-00001-deployment`.

image::greeter-00001.png[Greeter Service]

[#scaling-invoke-service]
== Invoke Service

include::partial$invoke-service.adoc[tags=*]

The last `curl` command should return a response like **Hi greeter => greeter-00001-deployment-5d696cc6c8-m65s5: 1**

Check <<basics-see-what-you-have-deployed,deployed Knative resources>> for more details on which Knative objects and resources have been created with the service deployment above.

[#scaling-scale-to-zero]
== Configure scale to zero 

Assuming that <<scaling-deploy-service, Greeter service>> has been deployed, once no more traffic is seen going into that service, we'd like to scale this service down to zero replicas. That's called **scale-to-zero**.

Scale-to-zero is one of the main properties making Knative a serverless platform. After a defined time of idleness (the so called `stable-window`) a revision is considered **inactive**. Now, all routes pointing to the now inactive revision will be pointed to the so-called **activator**. This reprogramming of the network is asynchronous in nature so the `scale-to-zero-grace-period` should give enough slack for this to happen. Once the `scale-to-zero-grace-period` is over, the revision will finally be scaled to zero replicas.

If another request tries to get to this revision, the activator will get it, instruct the autoscaler to create new pods for the revision as quickly as possible and buffer the request until those new pods are created.

[#scale-to-zero-formulae]
=== Calculating scale to zero time period

The Knative autoscaler uses the config map **config-autoscaler** in the **knative-serving** namespace for autoscaling related properties. The values of the attribute `stable-window` and `scale-to-zero-grace-period`, which we've seen above, are part of this config map.

[source,bash,subs="+macros,+attributes"]
----
STABLE_WINDOW=`kubectl -n knative-serving get configmaps config-autoscaler -o yaml |
 yq r - data.stable-window`
SCALE_TO_ZERO_GRACE_PERIOD=`kubectl -n knative-serving get configmaps config-autoscaler -o yaml |
 yq r - data.scale-to-zero-grace-period`
----

.(OR)

[source,bash,subs="+macros,+attributes",linenums]
----
STABLE_WINDOW=`oc -n knative-serving get configmaps config-autoscaler -o yaml |
 yq r - data.stable-window`
SCALE_TO_ZERO_GRACE_PERIOD=`oc -n knative-serving get configmaps config-autoscaler -o yaml |
 yq r - data.scale-to-zero-grace-period`
----

[.text-center]
As described above, the time it takes to finally scale to zero will be `STABLE_WINDOW + SCALE_TO_ZERO_GRACE_PERIOD`.

[IMPORTANT]
====
* scale-to-zero-grace-period is a dynamic parameter so the new value will immediately be in effect after updating the config map
* the minimum value that can be set for `scale-to-zero-grace-period` is 30 seconds
====

[#scaling-observer-scale-to-zero]
=== Observing default scale down 

For easier observation let us open a new terminal and run the following command,

[source,bash,subs="+macros,+attributes"]
----
watch 'oc get pods -n knativetutorial'
----

TIP: Up- and downscaling can also be watched on the OpenShift dashboard by navigating to the `knativetutorial` project

Checking default values
[source,bash,subs="+macros,+attributes",linenums]
----
# should return 60s
echo $STABLE_WINDOW 
# should return 30s
echo $SCALE_TO_ZERO_GRACE_PERIOD 
----

By default the **scale-to-zero-grace-period** is `30s`, and the **stable-window** is `60s`. Firing a request to the greeter service will bring up the pod (if it is already terminated, as described above) to serve the request. Leaving it without any further requests will automatically cause it to scale to zero in `1 min 30 secs`(<<scale-to-zero-formulae,Compute scale to zero grace period>>).

[#scaling-observer-scale-to-zero-1m]
=== Scale to zero in 1 minute

Let us now update **scale-to-zero-grace-period** to `1m` and leave the **stable-window** at the default `60s`.

Navigate to the tutorial chapter's `knative` folder:

[source,bash,subs="+macros,+attributes"]
----
cd $TUTORIAL_HOME/03-scaling/knative
----

[source,bash,subs="+macros,+attributes"]
----
kubectl -n knative-serving get cm config-autoscaler -o yaml | yq w - -s link:{github-repo}/{scaling-repo}/knative/configure-scaling-to-1m.yaml[configure-scaling-to-1m.yaml] | kubectl apply -f -
----

.(OR)

[source,bash,subs="+macros,+attributes"]
----
oc -n knative-serving get cm config-autoscaler -o yaml | yq w - -s link:{github-repo}/{scaling-repo}/knative/configure-scaling-to-1m.yaml[configure-scaling-to-1m.yaml] | oc apply -f -
----

Verifying the `scale-to-zero-grace-period` value, which should return `1m`: 

[source,bash,subs="+macros,+attributes"]
----
kubectl -n knative-serving get configmap config-autoscaler -o yaml \
 | yq r - data.scale-to-zero-grace-period
----

.(OR)

[source,bash,subs="+macros,+attributes"]
----
oc -n knative-serving get configmap config-autoscaler -o yaml \
  | yq r - data.scale-to-zero-grace-period
----

Now <<scaling-invoke-service,firing the request>> to the greeter service will bring up the pod to serve the request again. Leaving it without any further requests, it will automatically scale to zero in `2 mins`(<<scale-to-zero-formulae,Compute scale to zero grace period>>)

[#scaling-observer-scale-to-zero-2m]
=== Scale to zero in 2 minute

Let us now update **scale-to-zero-grace-period** to `2m` and leave the **stable-window** to default `60s`.

[source,bash,subs="+macros,+attributes"]
----
kubectl -n knative-serving get cm config-autoscaler -o yaml | yq w - -s link:{github-repo}/{scaling-repo}/knative/configure-scaling-to-2m.yaml[configure-scaling-to-2m.yaml] | kubectl apply -f -
----

.(OR)

[source,bash,subs="+macros,+attributes"]
----
oc -n knative-serving get cm config-autoscaler -o yaml | yq w - -s link:{github-repo}/{scaling-repo}/knative/configure-scaling-to-2m.yaml[configure-scaling-to-2m.yaml] | oc apply -f -
----

Verifying the `scale-to-zero-grace-period` value, which should return `2m`: 

[source,bash,subs="+macros,+attributes",linenums]
----

kubectl -n knative-serving get configmap config-autoscaler -o yaml \
 | yq r - data.scale-to-zero-grace-period
----

.(OR)

[source,bash,subs="+macros,+attributes",linenums]
----
oc -n knative-serving get configmap config-autoscaler -o yaml \
  | yq r - data.scale-to-zero-grace-period
----


Now <<scaling-invoke-service,firing the request>> to the greeter service will bring up the pod to serve the request and if we leave the service without any further requests, it will automatically scale to zero in `3 mins`(<<scale-to-zero-formulae,Compute scale to zero grace period>>).

[#scaling-reset-to-defaults]
=== Reset scale to zero to defaults

Let us revert the `scale-to-zero-grace-period` to its default:

[source,bash,subs="+macros,+attributes"]
----
kubectl -n knative-serving get cm config-autoscaler -o yaml | yq w - -s link:{github-repo}/{scaling-repo}/knative/configure-scaling-to-30s.yaml[configure-scaling-to-30s.yaml] | kubectl apply -f -
----

.(OR)

[source,bash,subs="+macros,+attributes"]
----
oc -n knative-serving get cm config-autoscaler -o yaml | yq w - -s link:{github-repo}/{scaling-repo}/knative/configure-scaling-to-30s.yaml[configure-scaling-to-30s.yaml] | oc apply -f -
----


Verifying the `scale-to-zero-grace-period` value, which should return `30s`: 

[source,bash,subs="+macros,+attributes",linenums]
----

kubectl -n knative-serving get configmap config-autoscaler -o yaml \
 | yq r - data.scale-to-zero-grace-period
----


.(OR)

[source,bash,subs="+macros,+attributes",linenums]
----
oc -n knative-serving get configmap config-autoscaler -o yaml \
  | yq r - data.scale-to-zero-grace-period
----

For better clarity and understanding let us <<scaling-cleanup,clean up>> the deployed knative resources before going to next section.

[#scaling-auto-scaling]
== Auto Scaling

By default Knative Serving allows 100 concurrent requests into a pod. This is defined by the `container-concurrency-target-default` setting in the configmap **config-autoscaler ** in the **knative-serving** namespace.

For this exercise let us make our service handle only **10** concurrent requests. This will cause Knative autoscaler to scale to more pods as soon as we run more than 10 requests in parallel against the revision.

[#scaling-concurrency-10]
=== Service with concurrency of 10 requests
[.text-center]
.link:{github-repo}/{scaling-repo}/knative/service-10.yaml[service-10.yaml]
[source,yaml,linenums]
----
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: greeter
  annotations:    
    autoscaling.knative.dev/target: "10" #<1>
spec:
  runLatest:
    configuration:
      revisionTemplate:
        spec:
          container:
            image: dev.local/rhdevelopers/greeter:0.0.1
            livenessProbe:
              httpGet:
                path: /healthz
            readinessProbe:
              httpGet:
                path: /healthz
----

<1> Will allow each service pod to handle max of 10 in-flight requests per pod before automatically scaling to new pod(s)

[#scaling-autoscaling-deploy-service]
=== Deploy service 

[source,bash,subs="+macros,+attributes"]
----
kubectl apply -n knativetutorial -f link:{github-repo}/{scaling-repo}/knative/service-10.yaml[service-10.yaml]
----

.(OR)

[source,bash,subs="+macros,+attributes"]
----
oc apply -n knativetutorial -f link:{github-repo}/{scaling-repo}/knative/service-10.yaml[service-10.yaml]
----

[#scaling-autoscaling-invoke-service]
=== Invoke Service 

include::partial$invoke-service.adoc[tag=env]

Open a new terminal and run the following command:

[source,bash,subs="+macros,+attributes"]
----
watch kubectl get pods -n knativetutorial
----

.(OR)

[source,bash,subs="+macros,+attributes"]
----
watch oc get pods -n knativetutorial
----

[#scaling-load-service]
=== Load the service

We will now send some load to the greeter service.  The command below sends 50 concurrent requests (`-c 50`) for the next 10s (`-z 10s`) with no connection timeout(`-t 0`)

[source,bash,subs="+macros,+attributes",linenums]
----
hey -z 10s -c 50 -t 0 \
  -host "greeter.knativetutorial.example.com" \
  "http://${IP_ADDRESS}"
----

After you√Ñ've successfully run this small load test, you will notice the number of greeter service pods will have scaled to 5 automatically. You can also view the same via OpenShift dashboard as shown below:

image::greeter-00001-scaled.png[Greeter Service]

The autoscale pods is computed using the formula:

[.text-center]
[source,bash,subs="+macros,+attributes"]
----
totalPodsToScale = inflightRequests / concurrencyTarget
----

With this current setting of **concurrencyTarget=10**(`containerConcurrency: "10"`) and **inflightRequests=50** , you will see Knative automatically scales the greeter services to  `**50/10 = 5 pods**`.

For more clarity and understanding let us <<scaling-cleanup,clean up>> existing deployments before proceeding to next section.

[#scaling-min-scale]
== Minimum Scale

In real world scenarios your service might need to handle sudden spikes in requests. Knative starts each service with a default of **1** replica. As described above, this will eventually be scaled to zero as described above. If your app needs to stay particularly responsive under any circumstances and/or has a slow startup time, it might be beneficial to always keep a minimum amount of pods around. This can be done via an the annotation **autoscaling.knative.dev/minScale**.

The following example shows how to make Knative create services that start with a replica count of **2** and never scale below it.

[#scaling-deploy-service-minscale]
=== Deploy service 
[.text-center]
.link:{github-repo}/{scaling-repo}/knative/service-min-scale.yaml[service-min-scale.yaml]
[source,yaml,linenums]
----
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: greeter
spec:
  runLatest:
    configuration:
      revisionTemplate:
        metadata:
          annotations:
            autoscaling.knative.dev/minScale: "2" #<1>
            # Target 10 in-flight-requests per pod.
            autoscaling.knative.dev/target: "10" #<2>
        spec:
          containerConcurrency: 10
          container:
            image: dev.local/rhdevelopers/greeter:0.0.1
            livenessProbe:
              httpGet:
                path: /healthz
            readinessProbe:
              httpGet:
                path: /healthz
----

<1> Will allow each service pod to handle max of 10 in-flight requests per pod before automatically scaling to new pods.
<2> The deployment of this service will always have a minimum of 2 pods.

[source,bash,subs="+macros,+attributes"]
----
kubectl apply -n knativetutorial -f link:{github-repo}/{scaling-repo}/knative/service-min-scale.yaml[service-min-scale.yaml]
----

.(OR)

[source,bash,subs="+macros,+attributes"]
----
oc apply -n knativetutorial -f link:{github-repo}/{scaling-repo}/knative/service-min-scale.yaml[service-min-scale.yaml]
----

After the deployment was successful we should see a kubernetes deployment called `greeter-00001-deployment` with **two** pods readily available.

image::greeter-00001-minscale.png[Greeter Service]

[.text-center]
**OpenShift Dashboard Knative Tutorial project**

Open a new terminal and run the following command :

[source,bash,subs="+macros,+attributes"]
----
watch kubectl get pods -n knativetutorial
----

.(OR)

[source,bash,subs="+macros,+attributes"]
----
watch oc get pods -n knativetutorial
----

Let us <<scaling-load-service,send some load to the service>> to trigger autoscaling.

When all requests are done and if we are beyond the `scale-to-zero-grace-period`, we will notice that Knative has terminated only 3 out 5 pods. This is because we have configured Knative to always run two pods via the annotation `autoscaling.knative.dev/minScale: "2"`.

[#scaling-cleanup]
== Cleanup

[source,bash,subs="+macros,+attributes"]
----
kubectl -n knativetutorial delete services.serving.knative.dev greeter
----

.(OR)

[source,bash,subs="+macros,+attributes"]
----
oc -n knativetutorial delete services.serving.knative.dev greeter
----
